engine:
    type: GlobusComputeEngine

    # Each worker has access to one tile
    available_accelerators: ["0.0","0.1","1.0","1.1","2.0","2.1","3.0","3.1","4.0","4.1","5.0","5.1"]
    cpu_affinity: block  # Assigns cpus in sequential order                                                                 
    prefetch_capacity: 0  # Increase if you have many more tasks than workers                                                    
    cores_per_worker: 16 # How many workers per core dictates total workers per node    


    strategy:
        type: SimpleStrategy
        max_idletime: 300

    address:
        type: address_by_interface
        ifname: bond0

    provider:
        type: PBSProProvider

        launcher:
            type: MpiExecLauncher
            # Ensures 1 manger per node, work on all 64 cores
            bind_cmd: --cpu-bind
            overrides: --depth=208 --ppn 1

        account: Aurora_deployment
        queue: workq
        cpus_per_node: 208
        select_options: "system=sunspot,place=scatter"

        # Node setup: activate necessary conda environment and such
        worker_init: "$HOME/LLM-service-api/funcx_service_13b/worker_init.sh"

        walltime: 00:15:00 
        nodes_per_block: 1 # This is the number of nodes in a batch job that the endpoint will submit 
        init_blocks: 0
        min_blocks: 0 # This is the minimum number of batch jobs the endpoint will run; as it is set to 0, it will not submit any batch jobs until it has work.
        max_blocks: 1 # This is the maximum number of batch jobs the endpoint will allow to run at one time; the number of batch jobs it will submit depends on how many tasks it has to run
	
